# ============================================================
# OpenNMT-py Transformer Training Config (Optimized for L4 GPU)
# ============================================================

## 1. Data and Vocabulary
data:
  corpus_1:
    path_src: data/train.src
    path_tgt: data/train.tgt
  valid:
    path_src: data/valid.src
    path_tgt: data/valid.tgt

save_data: data/vocab
src_vocab: data/vocab.src
tgt_vocab: data/vocab.tgt
src_vocab_size: 15000
tgt_vocab_size: 15000
share_vocab: false

## 2. Model Architecture (Transformer-Small)
model_type: text
encoder_type: transformer
decoder_type: transformer
word_vec_size: 256
rnn_size: 256
transformer_ff: 512
heads: 4
enc_layers: 4
dec_layers: 4
dropout: 0.2
position_encoding: true

## 3. Training & Optimization
world_size: 1
gpu_ranks: [0]
seed: 42
dtype: bf16                   # L4 Optimization for max speed/efficiency

batch_size: 6144              # Safe token batch size for 24GB VRAM
batch_type: tokens
normalization: tokens
accum_count: [1]
valid_batch_size: 1024

optim: adam
adam_beta2: 0.998
learning_rate: 1.5            # Conservative LR for small model
decay_method: noam
warmup_steps: 3000
label_smoothing: 0.1
max_grad_norm: 0.0
param_init: 0.0
param_init_glorot: true       # Use Glorot/Xavier init (works best w/ Noam)

## 4. Checkpointing & Logging
save_model: outputs/model_transformer
save_checkpoint_steps: 500
keep_checkpoint: 3
train_steps: 30000
valid_steps: 250
report_every: 50

log_file: outputs/train_log.txt
tensorboard: true
tensorboard_log_dir: outputs/runs

## 5. Early Stopping
early_stopping: 5
early_stopping_criteria: ppl
