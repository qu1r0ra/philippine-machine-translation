# ============================================================
# OpenNMT-py training configuration for a small Transformer
# ============================================================

## Data settings
data:
  corpus_1:
    path_src: data/train.src
    path_tgt: data/train.tgt
  valid:
    path_src: data/valid.src
    path_tgt: data/valid.tgt

## Vocabulary (built automatically before training)
save_data: data/vocab        # prefix for saved vocab file
src_vocab_size: 32000        # adjust if corpus is small
tgt_vocab_size: 32000
share_vocab: false           # set true for closely related languages

## Model checkpointing
save_model: outputs/model_transformer
save_checkpoint_steps: 5000
keep_checkpoint: 3
train_steps: 50000           # for small datasets, ~10kâ€“50k is reasonable
valid_steps: 1000
report_every: 100

## Model architecture
model_type: text
encoder_type: transformer
decoder_type: transformer

# --- Transformer architecture ---
word_vec_size: 256
rnn_size: 256
transformer_ff: 512
heads: 4
enc_layers: 4
dec_layers: 4
dropout: 0.1
position_encoding: true

## Training hyperparameters
optim: adam
adam_beta2: 0.998
learning_rate: 2.0
warmup_steps: 8000
decay_method: noam
label_smoothing: 0.1
max_grad_norm: 0.0
batch_size: 4096
batch_type: tokens
normalization: tokens
accum_count: [2]       # gradient accumulation for stability
valid_batch_size: 1024
param_init: 0.0
param_init_glorot: true

## GPU and seed
world_size: 1
gpu_ranks: [0]
seed: 42

## Logging and output
log_file: outputs/train_log.txt
tensorboard: true
tensorboard_log_dir: outputs/runs
