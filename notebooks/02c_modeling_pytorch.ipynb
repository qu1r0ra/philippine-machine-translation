{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qu1r0ra/philippine-machine-translation/blob/chore%2Fpolish-files/notebooks/02c_modeling_cbk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fippdg-rdIgY"
      },
      "source": [
        "# Modeling with PyTorch and not OpenNMT-py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "The author who trained the GRU model (initially Transformer) did not have sufficient computational resources to train locally, so he decided to train it via Google Colab. Unfortunately, the free plan in Colab also did not suffice for training as even the free T4 GPU has rate limits for free users, so the author also decided to purchase compute units to access Colab GPUs. You may also need Colab compute units to replicate the results of this notebook.\n",
        "\n",
        "If you decide to do so, you can press the '**Open in Colab**' button found at the topmost cell of this notebook to be led to Google Colab.\n",
        "\n",
        "Once you have opened this notebook in Colab, connect to a T4 GPU runtime. You don't need High RAM for this. You may choose other GPUs, but the authors found out through experimentation that the T4 is the most cost-efficient yet sufficient for this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pixU6GCKRDXs"
      },
      "source": [
        "### Acceptance Stage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjHewjxvOT3o"
      },
      "source": [
        "So here we are with yet another pivot because of all the problems we experiencing with `OpenNMT-py`. I am never ever using it again.\n",
        "\n",
        "Lesson learned: `PyTorch` is inevitable. Or perhaps I should've used `spaCy` instead. \\- CJ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WG0Z9MBRGrR"
      },
      "source": [
        "Anyways, let's load the data we will be training our models on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0151E7_Q3KR"
      },
      "outputs": [],
      "source": [
        "%mkdir data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UknRArE6RMMW"
      },
      "source": [
        "Upload your processed parallel corpora to separate folders in `data/`, each consisting of the ff.:\n",
        "- train.src (training set for source)\n",
        "- train.tgt (training set for target)\n",
        "- valid.src (validation set for source)\n",
        "- valid.tgt (validation set for target)\n",
        "\n",
        "These can be found in `data/processed/<model-name>`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34fu7RpSYNOH"
      },
      "source": [
        "Or if you have the data saved somewhere in your Google Drive with the same structure as above, load it from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W4ueescZMOs",
        "outputId": "4e181e8d-bc16-4fd8-e383-8147e6a848ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Change the variable below to the path containing your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiNCzDAcZS0A"
      },
      "outputs": [],
      "source": [
        "# Path to data folder, revise accordingly\n",
        "DATA_PATH = \"/content/drive/MyDrive/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmHT4Eq_Y4y3"
      },
      "source": [
        "## PyTorch implementation of our GRU model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_UZvKUxcFfL"
      },
      "source": [
        "Let's define the model's architecture through PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA5-2pZiaprc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ==== CONFIG ====\n",
        "EMB_DIM = 300\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PATIENCE = 3\n",
        "\n",
        "# ==== TOKENIZATION & ENCODING ====\n",
        "def tokenize(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "def encode(sentence, vocab):\n",
        "    return [vocab[\"<sos>\"]] + [vocab.get(w, len(vocab)) for w in tokenize(sentence)] + [vocab[\"<eos>\"]]\n",
        "\n",
        "# ==== DATASET & DATALOADER ====\n",
        "class MTDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
        "        self.data = [(encode(s, src_vocab), encode(t, tgt_vocab)) for s, t in pairs]\n",
        "    def __len__(self): return len(self.data)\n",
        "    def __getitem__(self, idx): return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_pad = nn.utils.rnn.pad_sequence([torch.tensor(s) for s in srcs], batch_first=True)\n",
        "    tgt_pad = nn.utils.rnn.pad_sequence([torch.tensor(t) for t in tgts], batch_first=True)\n",
        "    return src_pad, tgt_pad\n",
        "\n",
        "def create_dataloaders(train_pairs, valid_pairs, src_vocab, tgt_vocab):\n",
        "    train_loader = DataLoader(MTDataset(train_pairs, src_vocab, tgt_vocab), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(MTDataset(valid_pairs, src_vocab, tgt_vocab), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "    return train_loader, valid_loader\n",
        "\n",
        "# ==== MODEL ====\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        hidden = hidden[-1].unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.GRU(hid_dim + emb_dim, hid_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim * 2 + emb_dim, output_dim)\n",
        "        self.attention = Attention(hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(1)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        attn = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
        "        context = attn.bmm(encoder_outputs)\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        prediction = self.fc_out(torch.cat((output, context, embedded), dim=2).squeeze(1))\n",
        "        return prediction, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "    def forward(self, src, tgt):\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        input_tok = tgt[:, 0]\n",
        "        outputs = []\n",
        "        for t in range(1, tgt.size(1)):\n",
        "            output, hidden = self.decoder(input_tok, hidden, encoder_outputs)\n",
        "            outputs.append(output.unsqueeze(1))\n",
        "            input_tok = tgt[:, t]\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "# ==== LOAD DATA ====\n",
        "def load_parallel_corpus(folder_path):\n",
        "    def read_file(path):\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            return [line.strip() for line in f]\n",
        "    train_src = read_file(os.path.join(folder_path, \"train.src\"))\n",
        "    train_tgt = read_file(os.path.join(folder_path, \"train.tgt\"))\n",
        "    valid_src = read_file(os.path.join(folder_path, \"valid.src\"))\n",
        "    valid_tgt = read_file(os.path.join(folder_path, \"valid.tgt\"))\n",
        "    return list(zip(train_src, train_tgt)), list(zip(valid_src, valid_tgt))\n",
        "\n",
        "# ==== TRAINING ====\n",
        "def train_model(folder_name, train_pairs, valid_pairs):\n",
        "    # Build vocab\n",
        "    src_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2}\n",
        "    tgt_vocab = {\"<pad>\":0, \"<sos>\":1, \"<eos>\":2}\n",
        "    for src, tgt in train_pairs + valid_pairs:\n",
        "        for word in tokenize(src): src_vocab.setdefault(word, len(src_vocab))\n",
        "        for word in tokenize(tgt): tgt_vocab.setdefault(word, len(tgt_vocab))\n",
        "    src_ivocab = {v:k for k,v in src_vocab.items()}\n",
        "    tgt_ivocab = {v:k for k,v in tgt_vocab.items()}\n",
        "\n",
        "    train_loader, valid_loader = create_dataloaders(train_pairs, valid_pairs, src_vocab, tgt_vocab)\n",
        "\n",
        "    enc = Encoder(len(src_vocab), EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "    dec = Decoder(len(tgt_vocab), EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for src, tgt in train_loader:\n",
        "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt)\n",
        "            loss = criterion(output.view(-1, output.size(-1)), tgt[:,1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in valid_loader:\n",
        "                src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
        "                output = model(src, tgt)\n",
        "                loss = criterion(output.view(-1, output.size(-1)), tgt[:,1:].contiguous().view(-1))\n",
        "                val_loss += loss.item()\n",
        "        avg_val_loss = val_loss / len(valid_loader)\n",
        "\n",
        "        print(f\"[{folder_name}] Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Valid Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Save checkpoint with vocabs\n",
        "        if avg_val_loss < best_valid_loss:\n",
        "            best_valid_loss = avg_val_loss\n",
        "            epochs_no_improve = 0\n",
        "            save_path = f\"outputs/gru_{folder_name}_model.pt\"\n",
        "            torch.save({\n",
        "                'model_state': model.state_dict(),\n",
        "                'src_vocab': src_vocab,\n",
        "                'tgt_vocab': tgt_vocab,\n",
        "                'tgt_ivocab': tgt_ivocab\n",
        "            }, save_path)\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"[{folder_name}] Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    return model, src_vocab, tgt_vocab, src_ivocab, tgt_ivocab\n",
        "\n",
        "# ==== TRANSLATION ====\n",
        "def translate(model, sentence, src_vocab, tgt_vocab, tgt_ivocab, max_len=50):\n",
        "    \"\"\"\n",
        "    Translate a single sentence using a trained Seq2Seq model.\n",
        "\n",
        "    Unknown source words are mapped to <eos> (or <pad> if preferred).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    # Encode sentence: unknown words map to <eos>\n",
        "    src_indices = [src_vocab[\"<sos>\"]] + [src_vocab.get(w, src_vocab[\"<eos>\"]) for w in sentence.lower().split()] + [src_vocab[\"<eos>\"]]\n",
        "    src = torch.tensor(src_indices).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs, hidden = model.encoder(src)\n",
        "        input_tok = torch.tensor([tgt_vocab[\"<sos>\"]]).to(DEVICE)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            output, hidden = model.decoder(input_tok, hidden, encoder_outputs)\n",
        "            token = output.argmax(1).item()\n",
        "            if token == tgt_vocab[\"<eos>\"]:\n",
        "                break\n",
        "            # Map token id back to word\n",
        "            result.append(tgt_ivocab.get(token, \"<unk>\"))\n",
        "            input_tok = torch.tensor([token]).to(DEVICE)\n",
        "\n",
        "    return \" \".join(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91DFwZMvtoxJ",
        "outputId": "23eb7ccd-0344-4085-f6aa-07dd267b3001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing folder: aug-cbk ===\n",
            "[aug-cbk] Epoch 1/20 | Train Loss: 5.5122 | Valid Loss: 4.8026\n",
            "[aug-cbk] Epoch 2/20 | Train Loss: 4.2875 | Valid Loss: 4.2235\n",
            "[aug-cbk] Epoch 3/20 | Train Loss: 3.5322 | Valid Loss: 3.9153\n",
            "[aug-cbk] Epoch 4/20 | Train Loss: 2.9487 | Valid Loss: 3.7305\n",
            "[aug-cbk] Epoch 5/20 | Train Loss: 2.5212 | Valid Loss: 3.6274\n",
            "[aug-cbk] Epoch 6/20 | Train Loss: 2.2155 | Valid Loss: 3.5877\n",
            "[aug-cbk] Epoch 7/20 | Train Loss: 1.9911 | Valid Loss: 3.5709\n",
            "[aug-cbk] Epoch 8/20 | Train Loss: 1.8183 | Valid Loss: 3.5670\n",
            "[aug-cbk] Epoch 9/20 | Train Loss: 1.6815 | Valid Loss: 3.5715\n",
            "[aug-cbk] Epoch 10/20 | Train Loss: 1.5714 | Valid Loss: 3.5973\n",
            "[aug-cbk] Epoch 11/20 | Train Loss: 1.4824 | Valid Loss: 3.6227\n",
            "[aug-cbk] Early stopping triggered at epoch 11\n",
            "[aug-cbk] Test Translation: espigó pues hasta el día de noche y cuando desgranó su rama y las echó sobre la cama y las unió a un efa\n",
            "\n",
            "=== Processing folder: base ===\n",
            "[base] Epoch 1/20 | Train Loss: 5.6917 | Valid Loss: 5.0164\n",
            "[base] Epoch 2/20 | Train Loss: 4.5419 | Valid Loss: 4.4826\n",
            "[base] Epoch 3/20 | Train Loss: 3.8469 | Valid Loss: 4.2488\n",
            "[base] Epoch 4/20 | Train Loss: 3.3187 | Valid Loss: 4.1686\n",
            "[base] Epoch 5/20 | Train Loss: 2.9016 | Valid Loss: 4.1266\n",
            "[base] Epoch 6/20 | Train Loss: 2.5800 | Valid Loss: 4.1124\n",
            "[base] Epoch 7/20 | Train Loss: 2.3313 | Valid Loss: 4.1198\n",
            "[base] Epoch 8/20 | Train Loss: 2.1309 | Valid Loss: 4.1460\n",
            "[base] Epoch 9/20 | Train Loss: 1.9723 | Valid Loss: 4.1551\n",
            "[base] Early stopping triggered at epoch 9\n",
            "[base] Test Translation: beben en las fornicaciones de los pobres y no se casarán con josé y josé no se afligen de josé\n",
            "\n",
            "=== Processing folder: aug-noise ===\n",
            "[aug-noise] Epoch 1/20 | Train Loss: 5.0591 | Valid Loss: 4.0763\n",
            "[aug-noise] Epoch 2/20 | Train Loss: 3.4662 | Valid Loss: 3.1327\n",
            "[aug-noise] Epoch 3/20 | Train Loss: 2.5819 | Valid Loss: 2.5901\n",
            "[aug-noise] Epoch 4/20 | Train Loss: 2.0886 | Valid Loss: 2.2664\n",
            "[aug-noise] Epoch 5/20 | Train Loss: 1.7852 | Valid Loss: 2.0608\n",
            "[aug-noise] Epoch 6/20 | Train Loss: 1.5851 | Valid Loss: 1.9057\n",
            "[aug-noise] Epoch 7/20 | Train Loss: 1.4380 | Valid Loss: 1.7983\n",
            "[aug-noise] Epoch 8/20 | Train Loss: 1.3291 | Valid Loss: 1.7167\n",
            "[aug-noise] Epoch 9/20 | Train Loss: 1.2424 | Valid Loss: 1.6541\n",
            "[aug-noise] Epoch 10/20 | Train Loss: 1.1743 | Valid Loss: 1.6035\n",
            "[aug-noise] Epoch 11/20 | Train Loss: 1.1140 | Valid Loss: 1.5487\n",
            "[aug-noise] Epoch 12/20 | Train Loss: 1.0677 | Valid Loss: 1.5200\n",
            "[aug-noise] Epoch 13/20 | Train Loss: 1.0271 | Valid Loss: 1.4937\n",
            "[aug-noise] Epoch 14/20 | Train Loss: 0.9926 | Valid Loss: 1.4688\n",
            "[aug-noise] Epoch 15/20 | Train Loss: 0.9624 | Valid Loss: 1.4572\n",
            "[aug-noise] Epoch 16/20 | Train Loss: 0.9353 | Valid Loss: 1.4231\n",
            "[aug-noise] Epoch 17/20 | Train Loss: 0.9108 | Valid Loss: 1.4002\n",
            "[aug-noise] Epoch 18/20 | Train Loss: 0.8921 | Valid Loss: 1.4085\n",
            "[aug-noise] Epoch 19/20 | Train Loss: 0.8744 | Valid Loss: 1.3748\n",
            "[aug-noise] Epoch 20/20 | Train Loss: 0.8578 | Valid Loss: 1.3742\n",
            "[aug-noise] Test Translation: y todo lo que no tiene aletas y escama os será inmundo\n"
          ]
        }
      ],
      "source": [
        "# ==== RUN TRAINING ====\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "for folder_name in os.listdir(DATA_PATH):\n",
        "    folder_path = os.path.join(DATA_PATH, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        print(f\"\\n=== Processing folder: {folder_name} ===\")\n",
        "        train_pairs, valid_pairs = load_parallel_corpus(folder_path)\n",
        "        model, src_vocab, tgt_vocab, src_ivocab, tgt_ivocab = train_model(folder_name, train_pairs, valid_pairs)\n",
        "        # Optional test translation for sanity check\n",
        "        test_sentence = train_pairs[0][0]\n",
        "        print(f\"[{folder_name}] Test Translation: {translate(model, test_sentence, src_vocab, tgt_vocab, tgt_ivocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PNmwUG-n9Gn"
      },
      "outputs": [],
      "source": [
        "# ==== TRANSLATE TEST CORPUS ====\n",
        "def translate_test_corpus(test_file=f\"{DATA_PATH}/test.src\", outputs_folder=\"outputs\", max_len=50):\n",
        "    \"\"\"\n",
        "    Translate all sentences in a test file using all .pt models in the outputs folder.\n",
        "    Saves translations to <model_name>_translations.txt.\n",
        "    \"\"\"\n",
        "    with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        test_sentences = [line.strip() for line in f]\n",
        "\n",
        "    for model_file in os.listdir(outputs_folder):\n",
        "        if model_file.endswith(\".pt\"):\n",
        "            # Load checkpoint\n",
        "            checkpoint = torch.load(os.path.join(outputs_folder, model_file), map_location=DEVICE)\n",
        "\n",
        "            src_vocab = checkpoint['src_vocab']\n",
        "            tgt_vocab = checkpoint['tgt_vocab']\n",
        "            tgt_ivocab = checkpoint['tgt_ivocab']\n",
        "\n",
        "            # Initialize model with exact vocab sizes from training\n",
        "            enc = Encoder(len(src_vocab), EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "            dec = Decoder(len(tgt_vocab), EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
        "            model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
        "            model.load_state_dict(checkpoint['model_state'])\n",
        "            model.eval()\n",
        "\n",
        "            output_file = os.path.join(outputs_folder, f\"{os.path.splitext(model_file)[0]}_translations.txt\")\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
        "                for sent in test_sentences:\n",
        "                    translation = translate(model, sent, src_vocab, tgt_vocab, tgt_ivocab, max_len=max_len)\n",
        "                    out_f.write(translation + \"\\n\")\n",
        "\n",
        "            print(f\"[{model_file}] Translations saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZCn9OqheQWX"
      },
      "source": [
        "## Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0ZDkJ0JeSeU",
        "outputId": "25f780ea-4272-44b4-bd2e-fc92358dbee4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[gru_base_model.pt] Translations saved to outputs/gru_base_model_translations.txt\n",
            "[gru_aug-noise_model.pt] Translations saved to outputs/gru_aug-noise_model_translations.txt\n",
            "[gru_aug-cbk_model.pt] Translations saved to outputs/gru_aug-cbk_model_translations.txt\n"
          ]
        }
      ],
      "source": [
        "translate_test_corpus()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmV6s8op3FuK"
      },
      "source": [
        "Let's save the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw4-Ldvr3IAd"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/pytorch_models\n",
        "!cp -r outputs /content/drive/MyDrive/pytorch_models/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMctc5jV/ihyg2N0wQx9jRP",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
