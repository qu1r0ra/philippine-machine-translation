{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd0ed826-5387-4413-aabd-cc7e3acc21e7",
   "metadata": {},
   "source": [
    "# philippine-machine-translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2ed62d-e4ec-4900-9ace-b4c7f26af98b",
   "metadata": {},
   "source": [
    "An exploration of neural machine translation for Cebuano-Spanish translation. Created for NLP1000 (Introduction to Natural Language Processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408466d7-22a7-48a9-a129-044163e754cd",
   "metadata": {},
   "source": [
    "## Notes and Prerequisites\n",
    "\n",
    "This project is divided into 4 notebooks:\n",
    "1. Setup (you are here!)\n",
    "2. Preprocessing\n",
    "3. Modeling\n",
    "4. Evaluation\n",
    "\n",
    "The notebooks should be executed in a sequential manner as listed above to achieve the desired results. The same goes for the cells in each notebook.\n",
    "\n",
    "It is expected that you have cloned the [GitHub repository](https://github.com/qu1r0ra/philippine-machine-translation) containing the notebooks as it contains project dependencies. If not, then please do so and run `uv sync` at the root to install the necessary dependencies.\n",
    "\n",
    "Most of the code used throughout the notebooks are abstracted away into the repository along with other files. This a deliberate decision made by the authors to ensure that the notebooks are as clean and readable as possible. In a sense, the notebooks can be thought of as the project's 'presentation layer' which utilizes functions, classes, and other entities contained in Python modules developed by the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182ad50-a975-4181-a733-fa8e5a570d1c",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c162c-eb26-41f9-be56-f7add83c36ff",
   "metadata": {},
   "source": [
    "Let us begin by setting up our notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b73b2-a056-4b8d-8538-532da2649d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2eab59-9d99-4831-bc17-a548ffeaef3f",
   "metadata": {},
   "source": [
    "Right above are simply Jupyter magic commands that made the authors' lives easier during development. You should try them too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f051b27-6aad-431b-99d6-8f33cf242e9f",
   "metadata": {},
   "source": [
    "Next, let us import the necessary libraries and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaea28f-1ac5-4532-b142-5b81c4f4e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from src.config import RAW_DIR\n",
    "from src.utils import extract_archives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c5faa-11bb-4432-b764-9ebd8455df7d",
   "metadata": {},
   "source": [
    "Imported modules prefixed with `src` are developed by the authors. The rest are third-party."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee3079-3e77-46c2-b6c8-a97aecb65eb7",
   "metadata": {},
   "source": [
    "The data that our project will be using is in `data\\raw\\parallel_corpora_by_verse.zip`. It is currently compressed as a `.zip` file to save space. Let us first extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6260786-12d8-4cd0-b0b3-6f7985f38edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_archives(RAW_DIR, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4104d1-a55a-4e71-a7aa-bcf0a8cf9e38",
   "metadata": {},
   "source": [
    "We should now have 8 `.csv` files in the same directory, one for each parallel corpus. Let us confirm this and view the first few lines of one of the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec50a40-a1a4-4fa5-8756-7e9e320ec488",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = list(RAW_DIR.glob(\"**/*.csv\"))\n",
    "\n",
    "if csv_files:\n",
    "    print(f\"Found {len(csv_files)} CSV files. \")\n",
    "    print(f\"Showing first few lines of '{csv_files[0].name}':\")\n",
    "    df = pd.read_csv(csv_files[0])\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"No CSV files found in extracted data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a06c5f-5d79-466d-8520-6388b6f03983",
   "metadata": {},
   "source": [
    "At this point, we can proceed with **preprocessing** knowing our environment has been set up and our data has been properly extracted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philippine-machine-translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
