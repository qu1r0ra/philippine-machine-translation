{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984eff8d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ff774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sacrebleu import CHRF, TER, corpus_bleu\n",
    "\n",
    "from src.config import RESULTS_DIR, TRANSLATIONS_DIR\n",
    "from src.utils import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2685c44",
   "metadata": {},
   "source": [
    "Upload the translations to `data/translations` and revise the filenames below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = load_file(\"data/test.tgt\")\n",
    "models = {\n",
    "    \"GRU-Base\": load_file(TRANSLATIONS_DIR / \"trans-base.txt\"),\n",
    "    \"GRU-Aug\": load_file(TRANSLATIONS_DIR / \"trans-aug.txt\"),\n",
    "    \"GRU-Aug-CBK\": load_file(TRANSLATIONS_DIR / \"trans-aug-cbk.txt\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4b14f",
   "metadata": {},
   "source": [
    "Let us compute for different evaluation metrics, namely BLEU, CHRF, and TER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "chrf = CHRF()\n",
    "ter = TER()\n",
    "\n",
    "for name, preds in models.items():\n",
    "    bleu = corpus_bleu(preds, [refs])\n",
    "    chrf = chrf.corpus_score(preds, [refs])\n",
    "    ter = ter.corpus_score(preds, [refs])\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Name\": name,\n",
    "            \"BLEU\": bleu.score,\n",
    "            \"CHRF\": chrf.score,\n",
    "            \"TER\": ter.score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"BLEU: {bleu.score:.2f}\")\n",
    "    print(f\"CHRF: {chrf.score:.2f}\")\n",
    "    print(f\"TER: {ter.score:.2f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efdfc8",
   "metadata": {},
   "source": [
    "Let us visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e86c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_df.plot(kind=\"bar\", figsize=(10, 5))\n",
    "plt.title(\"Translation Quality Comparison\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Metric\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdc3f1",
   "metadata": {},
   "source": [
    "Let us save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(RESULTS_DIR / \"metrics_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philippine-machine-translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
