{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "984eff8d",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ff774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sacrebleu import CHRF, TER, corpus_bleu\n",
    "\n",
    "from src.config import RESULTS_DIR, TRANSLATIONS_DIR\n",
    "from src.utils import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2685c44",
   "metadata": {},
   "source": [
    "Upload the resulting translations to `data/translations` and revise the filepaths below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = load_file(TRANSLATIONS_DIR / \"test.tgt\")\n",
    "models = {\n",
    "    \"GRU-Base\": load_file(TRANSLATIONS_DIR / \"base.txt\"),\n",
    "    \"GRU-Aug\": load_file(TRANSLATIONS_DIR / \"aug.txt\"),\n",
    "    \"GRU-Aug-CBK\": load_file(TRANSLATIONS_DIR / \"aug-cbk.txt\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd4b14f",
   "metadata": {},
   "source": [
    "Let's compute for different evaluation metrics, namely BLEU, CHRF, and TER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "chrf = CHRF()\n",
    "ter = TER()\n",
    "\n",
    "for name, preds in models.items():\n",
    "    bleu_score = corpus_bleu(preds, [refs])\n",
    "    chrf_score = chrf.corpus_score(preds, [refs])\n",
    "    ter_score = ter.corpus_score(preds, [refs])\n",
    "\n",
    "    results.append(\n",
    "        {\n",
    "            \"Model\": name,\n",
    "            \"BLEU\": bleu_score.score,\n",
    "            \"CHRF\": chrf_score.score,\n",
    "            \"TER\": ter_score.score,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[{name}]\")\n",
    "    print(f\"BLEU: {bleu_score.score:.2f}\")\n",
    "    print(f\"CHRF: {chrf_score.score:.2f}\")\n",
    "    print(f\"TER: {ter_score.score:.2f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results).set_index(\"Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efdfc8",
   "metadata": {},
   "source": [
    "Let's visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e86c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics = [\"BLEU\", \"CHRF\", \"TER\"]\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    results_df[metric].plot(\n",
    "        kind=\"bar\",\n",
    "        ax=axes[i],\n",
    "        title=f\"Translation Quality Comparison by {metric}\",\n",
    "    )\n",
    "\n",
    "    axes[i].set_xlabel(\"\")\n",
    "    axes[i].set_ylabel(\"Score\" if i == 0 else \"\")\n",
    "    axes[i].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "    # Annotation logic for adding values to the bars\n",
    "    for container in axes[i].containers:\n",
    "        for bar in container:\n",
    "            height = bar.get_height()\n",
    "            axes[i].annotate(\n",
    "                f\"{height:.2f}\",\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                xytext=(0, 3),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Translation Quality Comparisons of Different Models Across Different Metrics\",\n",
    "    y=1.04,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(RESULTS_DIR / \"translation_quality_comparisons.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacdc3f1",
   "metadata": {},
   "source": [
    "Let's save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31862e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(RESULTS_DIR / \"metrics_summary.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "philippine-machine-translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
